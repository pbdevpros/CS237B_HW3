{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "MaHj6n4KRzjC",
        "outputId": "679cd309-79e8-473e-e86c-721b7d3c9b57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MaHj6n4KRzjC",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/CS237B_HW3-P2ii\n",
        "%ls"
      ],
      "metadata": {
        "id": "636mOlCXSdOP",
        "outputId": "4af5eea6-01a0-4d23-ef2e-f92ba3b8ab0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "636mOlCXSdOP",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CS237B_HW3-P2ii\n",
            "\u001b[0m\u001b[01;34mcustom\u001b[0m/                 make_submission.sh  test_coil.py    train_ildist.py\n",
            "\u001b[01;34mdata\u001b[0m/                   play.py             test_ildist.py  train_il.py\n",
            "\u001b[01;34mgym_carlo\u001b[0m/              \u001b[01;34mpolicies\u001b[0m/           test_il.py      utils.py\n",
            "il_dist_notebook.ipynb  requirements.txt    \u001b[01;34mtex\u001b[0m/\n",
            "intent_inference.py     shared_autonomy.py  train_coil.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['DISPLAY']=':0.0'"
      ],
      "metadata": {
        "id": "Y6qP3zwnS3WL"
      },
      "id": "Y6qP3zwnS3WL",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from gym_carlo.envs.geometry import Point\n",
        "\n",
        "scenario_names = ['intersection', 'circularroad', 'lanechange']\n",
        "obs_sizes = {'intersection': 5, 'circularroad': 4, 'lanechange': 3}\n",
        "goals = {'intersection': ['left','straight','right'], 'circularroad': ['inner','outer'], 'lanechange': ['left','right']}\n",
        "steering_lims = {'intersection': [-0.5,0.5], 'circularroad': [-0.15,0.15], 'lanechange': [-0.15, 0.15]}\n",
        "\n",
        "def maybe_makedirs(path_to_create):\n",
        "    \"\"\"This function will create a directory, unless it exists already,\n",
        "    at which point the function will return.\n",
        "    The exception handling is necessary as it prevents a race condition\n",
        "    from occurring.\n",
        "    Inputs:\n",
        "        path_to_create - A string path to a directory you'd like created.\n",
        "    \"\"\"\n",
        "    try: \n",
        "        os.makedirs(path_to_create)\n",
        "    except OSError:\n",
        "        if not os.path.isdir(path_to_create):\n",
        "            raise\n",
        "\n",
        "\n",
        "def load_data(args):\n",
        "    data_name = args.goal.lower()\n",
        "    scenario_name = args.scenario.lower()\n",
        "      \n",
        "    assert scenario_name in goals.keys(), '--scenario argument is invalid!'\n",
        "    data = {}\n",
        "    if data_name == 'all':\n",
        "        np_data = [np.load('data/' + scenario_name + '_' + dn + '.npy') for dn in goals[scenario_name]]\n",
        "        u = np.vstack([np.ones((np_data[i].shape[0],1))*i for i in range(len(np_data))])\n",
        "        np_data = np.vstack(np_data)\n",
        "        data['u_train'] = np.array(u).astype('uint8').reshape(-1,1)\n",
        "    else:\n",
        "        assert data_name in goals[scenario_name], '--data argument is invalid!'\n",
        "        np_data = np.load('data/' + scenario_name + '_' + data_name + '.npy')\n",
        "\n",
        "    data['x_train'] = np_data[:,:-2].astype('float32')\n",
        "    data['y_train'] = np_data[:,-2:].astype('float32') # control is always 2D: throttle and steering\n",
        "    \n",
        "    return data\n",
        "    \n",
        "   \n",
        "def optimal_act_circularroad(env, d):\n",
        "    if env.ego.speed > 10:\n",
        "        throttle = 0.06 + np.random.randn()*0.02\n",
        "    else:\n",
        "        throttle = 0.6 + np.random.randn()*0.1\n",
        "        \n",
        "    # setting the steering is not fun. Let's practice some trigonometry\n",
        "    r1 = 30.0 # inner building radius (not used rn)\n",
        "    r2 = 39.2 # inner ring radius\n",
        "    R = 32.3 # desired radius\n",
        "    if d==1: R += 4.9\n",
        "    Rp = np.sqrt(r2**2 - R**2) # distance between current \"target\" point and the current desired point\n",
        "    theta = np.arctan2(env.ego.y - 60, env.ego.x - 60)\n",
        "    target = Point(60 + R*np.cos(theta) + Rp*np.cos(3*np.pi/2-theta), 60 + R*np.sin(theta) - Rp*np.sin(3*np.pi/2-theta)) # this is pure magic (or I need to draw it to explain)\n",
        "    desired_heading = np.arctan2(target.y - env.ego.y, target.x - env.ego.x) % (2*np.pi)\n",
        "    h = np.array([env.ego.heading, env.ego.heading - 2*np.pi])\n",
        "    hi = np.argmin(np.abs(desired_heading - h))\n",
        "    if desired_heading >= h[hi]: steering = 0.15 + np.random.randn()*0.05\n",
        "    else: steering = -0.15 + np.random.randn()*0.05\n",
        "    return np.array([steering, throttle]).reshape(1,-1)\n",
        "    \n",
        "    \n",
        "def optimal_act_lanechange(env, d):\n",
        "    if env.ego.speed > 10:\n",
        "        throttle = 0.06 + np.random.randn()*0.02\n",
        "    else:\n",
        "        throttle = 0.8 + np.random.randn()*0.1\n",
        "        \n",
        "    if d==0:\n",
        "        target = Point(37.55, env.ego.y + env.ego.speed*3)\n",
        "    elif d==1:\n",
        "        target = Point(42.45, env.ego.y + env.ego.speed*3)\n",
        "    desired_heading = np.arctan2(target.y - env.ego.y, target.x - env.ego.x) % (2*np.pi)\n",
        "    h = np.array([env.ego.heading, env.ego.heading - 2*np.pi])\n",
        "    hi = np.argmin(np.abs(desired_heading - h))\n",
        "    if desired_heading >= h[hi]: steering = 0.15 + np.random.randn()*0.05\n",
        "    else: steering = -0.15 + np.random.randn()*0.05\n",
        "    return np.array([steering, throttle]).reshape(1,-1)"
      ],
      "metadata": {
        "id": "GSnk_UQ0O77d"
      },
      "id": "GSnk_UQ0O77d",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow_probability import distributions as tfd\n",
        "import argparse\n",
        "# from utils import *\n",
        "\n",
        "tf.config.run_functions_eagerly(True)\n",
        "import tensorflow_probability as tfp\n",
        "tfd = tfp.distributions"
      ],
      "metadata": {
        "id": "iE2lWeSaO6u6"
      },
      "id": "iE2lWeSaO6u6",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c07555ce",
      "metadata": {
        "id": "c07555ce"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b9c52127",
      "metadata": {
        "id": "b9c52127"
      },
      "outputs": [],
      "source": [
        "def nn(data, args):\n",
        "    \"\"\"\n",
        "    Trains a feedforward NN. \n",
        "    \"\"\"\n",
        "    params = {\n",
        "        'train_batch_size': 4096*32,\n",
        "    }\n",
        "    in_size = data['x_train'].shape[-1]\n",
        "    out_size = data['y_train'].shape[-1]\n",
        "    \n",
        "    nn_model = NN(in_size, out_size)\n",
        "    if args.restore:\n",
        "        nn_model.load_weights('./policies/' + args.scenario.lower() + '_' + args.goal.lower() + '_ILDIST')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
        "\n",
        "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        ######### Your code starts here #########\n",
        "        # We want to perform a single training step (for one batch):\n",
        "        # 1. Make a forward pass through the model\n",
        "        # 2. Calculate the loss for the output of the forward pass\n",
        "        # 3. Based on the loss calculate the gradient for all weights\n",
        "        # 4. Run an optimization step on the weights.\n",
        "        # Helpful Functions: tf.GradientTape(), tf.GradientTape.gradient(), tf.keras.Optimizer.apply_gradients\n",
        "        with tf.GradientTape() as tape:\n",
        "            # forward pass\n",
        "            y_est = nn_model(x, training=True) # use dropout\n",
        "            # compute the loss\n",
        "            current_loss = loss(y_est, y)\n",
        "        grads = tape.gradient(current_loss, nn_model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(grads, nn_model.trainable_variables))\n",
        "        ########## Your code ends here ##########\n",
        "\n",
        "        train_loss(current_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def train(train_data):\n",
        "        for x, y in train_data:\n",
        "            train_step(x, y)\n",
        "\n",
        "\n",
        "    train_data = tf.data.Dataset.from_tensor_slices((data['x_train'], data['y_train'])).shuffle(100000).batch(params['train_batch_size'])\n",
        "\n",
        "    for epoch in range(args.epochs):\n",
        "        # Reset the metrics at the start of the next epoch\n",
        "        train_loss.reset_states()\n",
        "\n",
        "        train(train_data)\n",
        "\n",
        "        template = 'Epoch {}, Loss: {}'\n",
        "        print(template.format(epoch + 1, train_loss.result()))\n",
        "    nn_model.save_weights('./policies/' + args.scenario.lower() + '_' + args.goal.lower() + '_ILDIST')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23e6db4a",
      "metadata": {
        "id": "23e6db4a"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "33c1573e",
      "metadata": {
        "id": "33c1573e"
      },
      "outputs": [],
      "source": [
        "class MixtureDensityModelErrorFinal(tf.keras.losses.Loss):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MixtureDensityModelErrorFinal, self).__init__()\n",
        "\n",
        "    def call(self, y_true, y_pred, sample_weight=None):\n",
        "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "        self.z_mu = y_pred[:, :2]\n",
        "        self.z_sigma = y_pred[:, 2:]\n",
        "        epsilon = 0.00001\n",
        "        # print(self.z_mu.shape)\n",
        "        # print(self.z_sigma.shape)\n",
        "        # B, N = self.z_sigma.shape\n",
        "        # self.z_sigma = tf.reshape(self.z_sigma, (B, int(N/2), int(N/2)))\n",
        "        # covariance = self.z_sigma @ tf.transpose(self.z_sigma, perm=[0, 2, 1])\n",
        "        scale_tril = tfp.math.fill_triangular(self.z_sigma) + epsilon\n",
        "        # sigma = tf.matmul(scale_tril, tf.transpose(scale_tril, perm=[0, 2, 1]))\n",
        "        # print(covariance.shape)\n",
        "        mvn = tfd.MultivariateNormalTriL(loc=self.z_mu, scale_tril=scale_tril, allow_nan_stats=False)\n",
        "        # E = tf.reduce_mean(tf.math.log(mvn.prob(y_true)), 0)\n",
        "        E = tf.reduce_mean(mvn.log_prob(y_true), 0)\n",
        "        return -1 * E\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3edec096",
      "metadata": {
        "id": "3edec096"
      },
      "outputs": [],
      "source": [
        "def loss(y_est, y):\n",
        "    y = tf.cast(y, dtype=tf.float32)\n",
        "    ######### Your code starts here #########\n",
        "    # We want to compute the negative log-likelihood loss between y_est and y where\n",
        "    # - y_est is the output of the network for a batch of observations,\n",
        "    # - y is the actions the expert took for the corresponding batch of observations\n",
        "    # At the end your code should return the scalar loss value.\n",
        "    # HINT: You may find the classes of tensorflow_probability.distributions (imported as tfd) useful.\n",
        "    #       In particular, you can use MultivariateNormalFullCovariance or MultivariateNormalTriL, but they are not the only way.\n",
        "    # loss_object = MixtureDensityModelError(num_means=2, num_kernels=3)\n",
        "    loss_object = MixtureDensityModelErrorFinal()\n",
        "    sample_weights = tf.constant(([0.8, 0.2]))\n",
        "    y = y * sample_weights\n",
        "    y_est = y_est * tf.constant(([0.8, 0.2, 0.1, 0.1, 0.1]))\n",
        "    return loss_object(y, y_est)\n",
        "    \n",
        "    ########## Your code ends here ##########"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a6b1d42",
      "metadata": {
        "id": "1a6b1d42"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9b7c8641",
      "metadata": {
        "id": "9b7c8641"
      },
      "outputs": [],
      "source": [
        "class NN(tf.keras.Model):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(NN, self).__init__()\n",
        "        \n",
        "        ######### Your code starts here #########\n",
        "        # We want to define and initialize the weights & biases of the neural network.\n",
        "        # - in_size is dim(O)\n",
        "        # - out_size is dim(A) = 2\n",
        "        # IMPORTANT: out_size is still 2 in this case, because the action space is 2-dimensional. But your network will output some other size as it is outputing a distribution!\n",
        "        # HINT: You should use either of the following for weight initialization:\n",
        "        #         - tf.keras.initializers.GlorotUniform (this is what we tried)\n",
        "        #         - tf.keras.initializers.GlorotNormal\n",
        "        #         - tf.keras.initializers.he_uniform or tf.keras.initializers.he_normal\n",
        "        self.internal_layers = [\n",
        "            tf.keras.layers.Flatten(),\n",
        "            tf.keras.layers.Dense(24, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu'),\n",
        "            # tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(24, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu'),\n",
        "            # tf.keras.layers.Dropout(0.2),\n",
        "            # tf.keras.layers.Dense(12, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu'),\n",
        "        ]\n",
        "        # num_outputs = (out_size + 2) * 3 # Removed after using only 6 ouputs\n",
        "        num_outputs = out_size + 3\n",
        "        self.layer_output = tf.keras.layers.Dense(num_outputs, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu')\n",
        "        ########## Your code ends here ##########\n",
        "\n",
        "    def call(self, x):\n",
        "        x = tf.cast(x, dtype=tf.float32)\n",
        "        ######### Your code starts here #########\n",
        "        # We want to perform a forward-pass of the network. Using the weights and biases, this function should give the network output for x where:\n",
        "        # x is a (?, |O|) tensor that keeps a batch of observations\n",
        "        # IMPORTANT: First two columns of the output tensor must correspond to the mean vector!\n",
        "        for i in range(len(self.internal_layers)):\n",
        "            layer = self.internal_layers[i]\n",
        "            x = layer(x)\n",
        "        return self.layer_output(x)\n",
        "        ########## Your code ends here ##########"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9de95d",
      "metadata": {
        "id": "2f9de95d"
      },
      "source": [
        "# Run Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ceb1fdaf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceb1fdaf",
        "outputId": "b23638da-2169-4c32-b6e4-e51eda8fa5a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 17686646784.0\n",
            "Epoch 2, Loss: 15420235776.0\n",
            "Epoch 3, Loss: 13385179136.0\n",
            "Epoch 4, Loss: 11576787968.0\n",
            "Epoch 5, Loss: 9983521792.0\n",
            "Epoch 6, Loss: 8603997184.0\n",
            "Epoch 7, Loss: 7401025536.0\n",
            "Epoch 8, Loss: 6355261952.0\n",
            "Epoch 9, Loss: 5447610880.0\n",
            "Epoch 10, Loss: 4662502400.0\n",
            "Epoch 11, Loss: 3985242112.0\n",
            "Epoch 12, Loss: 3405187584.0\n",
            "Epoch 13, Loss: 2906147072.0\n",
            "Epoch 14, Loss: 2480228864.0\n",
            "Epoch 15, Loss: 2117806720.0\n",
            "Epoch 16, Loss: 1811988864.0\n",
            "Epoch 17, Loss: 1555349504.0\n",
            "Epoch 18, Loss: 1341999616.0\n",
            "Epoch 19, Loss: 1168116224.0\n",
            "Epoch 20, Loss: 1027526400.0\n",
            "Epoch 21, Loss: 915140928.0\n",
            "Epoch 22, Loss: 826948928.0\n",
            "Epoch 23, Loss: 757987904.0\n",
            "Epoch 24, Loss: 704222912.0\n",
            "Epoch 25, Loss: 660549056.0\n",
            "Epoch 26, Loss: 624540672.0\n",
            "Epoch 27, Loss: 593717376.0\n",
            "Epoch 28, Loss: 567357760.0\n",
            "Epoch 29, Loss: 543985216.0\n",
            "Epoch 30, Loss: 523163968.0\n",
            "Epoch 31, Loss: 504637792.0\n",
            "Epoch 32, Loss: 488082272.0\n",
            "Epoch 33, Loss: 473267744.0\n",
            "Epoch 34, Loss: 459658016.0\n",
            "Epoch 35, Loss: 447393600.0\n",
            "Epoch 36, Loss: 436293536.0\n",
            "Epoch 37, Loss: 426065888.0\n",
            "Epoch 38, Loss: 416745920.0\n",
            "Epoch 39, Loss: 408310144.0\n",
            "Epoch 40, Loss: 400467584.0\n",
            "Epoch 41, Loss: 393336928.0\n",
            "Epoch 42, Loss: 386652000.0\n",
            "Epoch 43, Loss: 380497888.0\n",
            "Epoch 44, Loss: 374779488.0\n",
            "Epoch 45, Loss: 369413664.0\n",
            "Epoch 46, Loss: 364575008.0\n",
            "Epoch 47, Loss: 359992352.0\n",
            "Epoch 48, Loss: 355722144.0\n",
            "Epoch 49, Loss: 351703616.0\n",
            "Epoch 50, Loss: 347988512.0\n",
            "Epoch 51, Loss: 344481792.0\n",
            "Epoch 52, Loss: 341159232.0\n",
            "Epoch 53, Loss: 337903136.0\n",
            "Epoch 54, Loss: 334898816.0\n",
            "Epoch 55, Loss: 332029248.0\n",
            "Epoch 56, Loss: 329256288.0\n",
            "Epoch 57, Loss: 326530592.0\n",
            "Epoch 58, Loss: 324080416.0\n",
            "Epoch 59, Loss: 321625504.0\n",
            "Epoch 60, Loss: 319254080.0\n",
            "Epoch 61, Loss: 316938784.0\n",
            "Epoch 62, Loss: 314780736.0\n",
            "Epoch 63, Loss: 312614240.0\n",
            "Epoch 64, Loss: 310543584.0\n",
            "Epoch 65, Loss: 308549376.0\n",
            "Epoch 66, Loss: 306643552.0\n",
            "Epoch 67, Loss: 304717184.0\n",
            "Epoch 68, Loss: 302681344.0\n",
            "Epoch 69, Loss: 300547968.0\n",
            "Epoch 70, Loss: 298327072.0\n",
            "Epoch 71, Loss: 296210624.0\n",
            "Epoch 72, Loss: 294034720.0\n",
            "Epoch 73, Loss: 291732416.0\n",
            "Epoch 74, Loss: 289774592.0\n",
            "Epoch 75, Loss: 287843712.0\n",
            "Epoch 76, Loss: 286036640.0\n",
            "Epoch 77, Loss: 284214944.0\n",
            "Epoch 78, Loss: 282347136.0\n",
            "Epoch 79, Loss: 280486048.0\n",
            "Epoch 80, Loss: 278728480.0\n",
            "Epoch 81, Loss: 277020992.0\n",
            "Epoch 82, Loss: 275258432.0\n",
            "Epoch 83, Loss: 273580128.0\n",
            "Epoch 84, Loss: 271882208.0\n",
            "Epoch 85, Loss: 270197504.0\n",
            "Epoch 86, Loss: 268449248.0\n",
            "Epoch 87, Loss: 268062928.0\n",
            "Epoch 88, Loss: 266400096.0\n",
            "Epoch 89, Loss: 264655952.0\n",
            "Epoch 90, Loss: 263046432.0\n",
            "Epoch 91, Loss: 261418688.0\n",
            "Epoch 92, Loss: 259787152.0\n",
            "Epoch 93, Loss: 258186352.0\n",
            "Epoch 94, Loss: 257986080.0\n",
            "Epoch 95, Loss: 257656096.0\n",
            "Epoch 96, Loss: 255962160.0\n",
            "Epoch 97, Loss: 254287312.0\n",
            "Epoch 98, Loss: 252707840.0\n",
            "Epoch 99, Loss: 250866576.0\n",
            "Epoch 100, Loss: 250388592.0\n",
            "Epoch 101, Loss: 248577248.0\n",
            "Epoch 102, Loss: 246786992.0\n",
            "Epoch 103, Loss: 244991680.0\n",
            "Epoch 104, Loss: 243327728.0\n",
            "Epoch 105, Loss: 242974176.0\n",
            "Epoch 106, Loss: 242559824.0\n",
            "Epoch 107, Loss: 240763792.0\n",
            "Epoch 108, Loss: 239959072.0\n",
            "Epoch 109, Loss: 238167136.0\n",
            "Epoch 110, Loss: 236408496.0\n",
            "Epoch 111, Loss: 235675232.0\n",
            "Epoch 112, Loss: 233855552.0\n",
            "Epoch 113, Loss: 232169616.0\n",
            "Epoch 114, Loss: 230540304.0\n",
            "Epoch 115, Loss: 229820368.0\n",
            "Epoch 116, Loss: 229330208.0\n",
            "Epoch 117, Loss: 229847808.0\n",
            "Epoch 118, Loss: 228259152.0\n",
            "Epoch 119, Loss: 226692896.0\n",
            "Epoch 120, Loss: 225166240.0\n",
            "Epoch 121, Loss: 223659760.0\n",
            "Epoch 122, Loss: 222165280.0\n",
            "Epoch 123, Loss: 221767232.0\n",
            "Epoch 124, Loss: 220404752.0\n",
            "Epoch 125, Loss: 219078704.0\n",
            "Epoch 126, Loss: 217757968.0\n",
            "Epoch 127, Loss: 216531840.0\n",
            "Epoch 128, Loss: 215178832.0\n",
            "Epoch 129, Loss: 213796688.0\n",
            "Epoch 130, Loss: 213644992.0\n",
            "Epoch 131, Loss: 212565936.0\n",
            "Epoch 132, Loss: 209508608.0\n",
            "Epoch 133, Loss: 203620256.0\n",
            "Epoch 134, Loss: 199369728.0\n",
            "Epoch 135, Loss: 197367056.0\n",
            "Epoch 136, Loss: 194283584.0\n",
            "Epoch 137, Loss: 193269872.0\n",
            "Epoch 138, Loss: 191326096.0\n",
            "Epoch 139, Loss: 190623200.0\n",
            "Epoch 140, Loss: 189923792.0\n",
            "Epoch 141, Loss: 189151424.0\n",
            "Epoch 142, Loss: 188185232.0\n",
            "Epoch 143, Loss: 187427216.0\n",
            "Epoch 144, Loss: 186706352.0\n",
            "Epoch 145, Loss: 186102144.0\n",
            "Epoch 146, Loss: 185435808.0\n",
            "Epoch 147, Loss: 184891696.0\n",
            "Epoch 148, Loss: 184471040.0\n",
            "Epoch 149, Loss: 184073248.0\n",
            "Epoch 150, Loss: 183651568.0\n",
            "Epoch 151, Loss: 183235120.0\n",
            "Epoch 152, Loss: 182790608.0\n",
            "Epoch 153, Loss: 182380384.0\n",
            "Epoch 154, Loss: 182029568.0\n",
            "Epoch 155, Loss: 181728208.0\n",
            "Epoch 156, Loss: 181454336.0\n",
            "Epoch 157, Loss: 181198352.0\n",
            "Epoch 158, Loss: 180987232.0\n",
            "Epoch 159, Loss: 180724064.0\n",
            "Epoch 160, Loss: 180273872.0\n",
            "Epoch 161, Loss: 179915184.0\n",
            "Epoch 162, Loss: 179653920.0\n",
            "Epoch 163, Loss: 179378384.0\n",
            "Epoch 164, Loss: 179150688.0\n",
            "Epoch 165, Loss: 178893072.0\n",
            "Epoch 166, Loss: 178656784.0\n",
            "Epoch 167, Loss: 178295584.0\n",
            "Epoch 168, Loss: 177851104.0\n",
            "Epoch 169, Loss: 177340880.0\n",
            "Epoch 170, Loss: 177018848.0\n",
            "Epoch 171, Loss: 176624192.0\n",
            "Epoch 172, Loss: 176277952.0\n",
            "Epoch 173, Loss: 175942576.0\n",
            "Epoch 174, Loss: 175650496.0\n",
            "Epoch 175, Loss: 175356448.0\n",
            "Epoch 176, Loss: 175043392.0\n",
            "Epoch 177, Loss: 174689120.0\n",
            "Epoch 178, Loss: 174337248.0\n",
            "Epoch 179, Loss: 174034768.0\n",
            "Epoch 180, Loss: 173776448.0\n",
            "Epoch 181, Loss: 173508704.0\n",
            "Epoch 182, Loss: 173147600.0\n",
            "Epoch 183, Loss: 172852400.0\n",
            "Epoch 184, Loss: 172619616.0\n",
            "Epoch 185, Loss: 172452032.0\n",
            "Epoch 186, Loss: 172198432.0\n",
            "Epoch 187, Loss: 172024208.0\n",
            "Epoch 188, Loss: 171880064.0\n",
            "Epoch 189, Loss: 171711408.0\n",
            "Epoch 190, Loss: 171528912.0\n",
            "Epoch 191, Loss: 171260608.0\n",
            "Epoch 192, Loss: 170935344.0\n",
            "Epoch 193, Loss: 170694928.0\n",
            "Epoch 194, Loss: 170455456.0\n",
            "Epoch 195, Loss: 170083520.0\n",
            "Epoch 196, Loss: 169857888.0\n",
            "Epoch 197, Loss: 169483664.0\n",
            "Epoch 198, Loss: 169095200.0\n",
            "Epoch 199, Loss: 168754464.0\n",
            "Epoch 200, Loss: 168497040.0\n",
            "Epoch 201, Loss: 168238832.0\n",
            "Epoch 202, Loss: 167940704.0\n",
            "Epoch 203, Loss: 167446640.0\n",
            "Epoch 204, Loss: 167077136.0\n",
            "Epoch 205, Loss: 166619408.0\n",
            "Epoch 206, Loss: 166082672.0\n",
            "Epoch 207, Loss: 165608512.0\n",
            "Epoch 208, Loss: 165224816.0\n",
            "Epoch 209, Loss: 164914272.0\n",
            "Epoch 210, Loss: 164411504.0\n",
            "Epoch 211, Loss: 163954640.0\n",
            "Epoch 212, Loss: 163543024.0\n",
            "Epoch 213, Loss: 163163296.0\n",
            "Epoch 214, Loss: 162809776.0\n",
            "Epoch 215, Loss: 162459696.0\n",
            "Epoch 216, Loss: 162146176.0\n",
            "Epoch 217, Loss: 161825024.0\n",
            "Epoch 218, Loss: 161601568.0\n",
            "Epoch 219, Loss: 161254496.0\n",
            "Epoch 220, Loss: 160938784.0\n",
            "Epoch 221, Loss: 160717888.0\n",
            "Epoch 222, Loss: 160308720.0\n",
            "Epoch 223, Loss: 159930448.0\n",
            "Epoch 224, Loss: 159614288.0\n",
            "Epoch 225, Loss: 159358432.0\n",
            "Epoch 226, Loss: 159239760.0\n",
            "Epoch 227, Loss: 159117040.0\n",
            "Epoch 228, Loss: 158903584.0\n",
            "Epoch 229, Loss: 158693216.0\n",
            "Epoch 230, Loss: 158471312.0\n",
            "Epoch 231, Loss: 158350208.0\n",
            "Epoch 232, Loss: 158137760.0\n",
            "Epoch 233, Loss: 157966128.0\n",
            "Epoch 234, Loss: 157574624.0\n",
            "Epoch 235, Loss: 157289136.0\n",
            "Epoch 236, Loss: 156998848.0\n",
            "Epoch 237, Loss: 156753200.0\n",
            "Epoch 238, Loss: 156560480.0\n",
            "Epoch 239, Loss: 156458848.0\n",
            "Epoch 240, Loss: 156260640.0\n",
            "Epoch 241, Loss: 156189024.0\n",
            "Epoch 242, Loss: 156041152.0\n",
            "Epoch 243, Loss: 155932752.0\n",
            "Epoch 244, Loss: 155841632.0\n",
            "Epoch 245, Loss: 155711264.0\n",
            "Epoch 246, Loss: 155647536.0\n",
            "Epoch 247, Loss: 155548816.0\n",
            "Epoch 248, Loss: 155449904.0\n",
            "Epoch 249, Loss: 155406400.0\n",
            "Epoch 250, Loss: 155383664.0\n",
            "Epoch 251, Loss: 155349088.0\n",
            "Epoch 252, Loss: 155297376.0\n",
            "Epoch 253, Loss: 155105936.0\n",
            "Epoch 254, Loss: 154895440.0\n",
            "Epoch 255, Loss: 154580864.0\n",
            "Epoch 256, Loss: 154346336.0\n",
            "Epoch 257, Loss: 154005664.0\n",
            "Epoch 258, Loss: 153681056.0\n",
            "Epoch 259, Loss: 153201952.0\n",
            "Epoch 260, Loss: 152845728.0\n",
            "Epoch 261, Loss: 152458656.0\n",
            "Epoch 262, Loss: 152237568.0\n",
            "Epoch 263, Loss: 151927808.0\n",
            "Epoch 264, Loss: 151679328.0\n",
            "Epoch 265, Loss: 151335760.0\n",
            "Epoch 266, Loss: 150990496.0\n",
            "Epoch 267, Loss: 150697952.0\n",
            "Epoch 268, Loss: 150419680.0\n",
            "Epoch 269, Loss: 149952064.0\n",
            "Epoch 270, Loss: 149353600.0\n",
            "Epoch 271, Loss: 148726112.0\n",
            "Epoch 272, Loss: 147966336.0\n",
            "Epoch 273, Loss: 147510272.0\n",
            "Epoch 274, Loss: 146763760.0\n",
            "Epoch 275, Loss: 146079024.0\n",
            "Epoch 276, Loss: 145372192.0\n",
            "Epoch 277, Loss: 144804560.0\n",
            "Epoch 278, Loss: 144146320.0\n",
            "Epoch 279, Loss: 143482432.0\n",
            "Epoch 280, Loss: 142790544.0\n",
            "Epoch 281, Loss: 142295248.0\n",
            "Epoch 282, Loss: 141827216.0\n",
            "Epoch 283, Loss: 141284272.0\n",
            "Epoch 284, Loss: 140769408.0\n",
            "Epoch 285, Loss: 139786176.0\n",
            "Epoch 286, Loss: 138887184.0\n",
            "Epoch 287, Loss: 138095296.0\n",
            "Epoch 288, Loss: 137386208.0\n",
            "Epoch 289, Loss: 136615104.0\n",
            "Epoch 290, Loss: 135818080.0\n",
            "Epoch 291, Loss: 135316288.0\n",
            "Epoch 292, Loss: 134742000.0\n",
            "Epoch 293, Loss: 134145552.0\n",
            "Epoch 294, Loss: 133507696.0\n",
            "Epoch 295, Loss: 132892600.0\n",
            "Epoch 296, Loss: 132417304.0\n",
            "Epoch 297, Loss: 131704384.0\n",
            "Epoch 298, Loss: 130990520.0\n",
            "Epoch 299, Loss: 130155816.0\n",
            "Epoch 300, Loss: 129332904.0\n",
            "Epoch 301, Loss: 128448552.0\n",
            "Epoch 302, Loss: 127753696.0\n",
            "Epoch 303, Loss: 126537728.0\n",
            "Epoch 304, Loss: 124412624.0\n",
            "Epoch 305, Loss: 122639616.0\n",
            "Epoch 306, Loss: 120826528.0\n",
            "Epoch 307, Loss: 119191712.0\n",
            "Epoch 308, Loss: 117688880.0\n",
            "Epoch 309, Loss: 116241944.0\n",
            "Epoch 310, Loss: 115041736.0\n",
            "Epoch 311, Loss: 113346504.0\n",
            "Epoch 312, Loss: 111617440.0\n",
            "Epoch 313, Loss: 109811680.0\n",
            "Epoch 314, Loss: 107860136.0\n",
            "Epoch 315, Loss: 106214200.0\n",
            "Epoch 316, Loss: 104654464.0\n",
            "Epoch 317, Loss: 103190984.0\n",
            "Epoch 318, Loss: 101617760.0\n",
            "Epoch 319, Loss: 100423576.0\n",
            "Epoch 320, Loss: 99231168.0\n",
            "Epoch 321, Loss: 98060160.0\n",
            "Epoch 322, Loss: 95883512.0\n",
            "Epoch 323, Loss: 94024544.0\n",
            "Epoch 324, Loss: 91920496.0\n",
            "Epoch 325, Loss: 90194104.0\n",
            "Epoch 326, Loss: 88288568.0\n",
            "Epoch 327, Loss: 86374352.0\n",
            "Epoch 328, Loss: 84359880.0\n",
            "Epoch 329, Loss: 82022744.0\n",
            "Epoch 330, Loss: 79810128.0\n",
            "Epoch 331, Loss: 77498704.0\n",
            "Epoch 332, Loss: 75248536.0\n",
            "Epoch 333, Loss: 72888064.0\n",
            "Epoch 334, Loss: 70544744.0\n",
            "Epoch 335, Loss: 68414608.0\n",
            "Epoch 336, Loss: 66341452.0\n",
            "Epoch 337, Loss: 64626836.0\n",
            "Epoch 338, Loss: 62657228.0\n",
            "Epoch 339, Loss: 60931736.0\n",
            "Epoch 340, Loss: 59153780.0\n",
            "Epoch 341, Loss: 56933692.0\n",
            "Epoch 342, Loss: 53759980.0\n",
            "Epoch 343, Loss: 51065224.0\n",
            "Epoch 344, Loss: 48789728.0\n",
            "Epoch 345, Loss: 46576804.0\n",
            "Epoch 346, Loss: 44055260.0\n",
            "Epoch 347, Loss: 41286608.0\n",
            "Epoch 348, Loss: 39103928.0\n",
            "Epoch 349, Loss: 37137992.0\n",
            "Epoch 350, Loss: 35225704.0\n",
            "Epoch 351, Loss: 33503588.0\n",
            "Epoch 352, Loss: 32121402.0\n",
            "Epoch 353, Loss: 30742194.0\n",
            "Epoch 354, Loss: 29538390.0\n",
            "Epoch 355, Loss: 28329226.0\n",
            "Epoch 356, Loss: 27240890.0\n",
            "Epoch 357, Loss: 26194206.0\n",
            "Epoch 358, Loss: 25315180.0\n",
            "Epoch 359, Loss: 24439994.0\n",
            "Epoch 360, Loss: 23771862.0\n",
            "Epoch 361, Loss: 23108254.0\n",
            "Epoch 362, Loss: 22561426.0\n",
            "Epoch 363, Loss: 21959296.0\n",
            "Epoch 364, Loss: 21333084.0\n",
            "Epoch 365, Loss: 20819632.0\n",
            "Epoch 366, Loss: 19703232.0\n",
            "Epoch 367, Loss: 18751992.0\n",
            "Epoch 368, Loss: 17782610.0\n",
            "Epoch 369, Loss: 16910074.0\n",
            "Epoch 370, Loss: 16021518.0\n",
            "Epoch 371, Loss: 15336948.0\n",
            "Epoch 372, Loss: 14603623.0\n",
            "Epoch 373, Loss: 13134607.0\n",
            "Epoch 374, Loss: 11945747.0\n",
            "Epoch 375, Loss: 10861470.0\n",
            "Epoch 376, Loss: 9870789.0\n",
            "Epoch 377, Loss: 9039873.0\n",
            "Epoch 378, Loss: 8278047.5\n",
            "Epoch 379, Loss: 7726173.5\n",
            "Epoch 380, Loss: 7084337.0\n",
            "Epoch 381, Loss: 6580899.5\n",
            "Epoch 382, Loss: 6075487.0\n",
            "Epoch 383, Loss: 5652003.0\n",
            "Epoch 384, Loss: 5244652.5\n",
            "Epoch 385, Loss: 4944571.0\n",
            "Epoch 386, Loss: 4538156.5\n",
            "Epoch 387, Loss: 4237187.5\n",
            "Epoch 388, Loss: 4008462.0\n",
            "Epoch 389, Loss: 3762710.0\n",
            "Epoch 390, Loss: 3526912.0\n",
            "Epoch 391, Loss: 3323476.75\n",
            "Epoch 392, Loss: 3182085.5\n",
            "Epoch 393, Loss: 3040608.5\n",
            "Epoch 394, Loss: 2895411.25\n",
            "Epoch 395, Loss: 2789424.0\n",
            "Epoch 396, Loss: 2638708.75\n",
            "Epoch 397, Loss: 2482803.25\n",
            "Epoch 398, Loss: 2364755.5\n",
            "Epoch 399, Loss: 2225893.5\n",
            "Epoch 400, Loss: 2123130.0\n",
            "Epoch 401, Loss: 2005215.25\n",
            "Epoch 402, Loss: 1953841.875\n",
            "Epoch 403, Loss: 1853057.125\n",
            "Epoch 404, Loss: 1759421.125\n",
            "Epoch 405, Loss: 1729101.5\n",
            "Epoch 406, Loss: 1674771.125\n",
            "Epoch 407, Loss: 1632424.625\n",
            "Epoch 408, Loss: 1547726.25\n",
            "Epoch 409, Loss: 1420891.625\n",
            "Epoch 410, Loss: 1304683.125\n",
            "Epoch 411, Loss: 1264102.5\n",
            "Epoch 412, Loss: 1231372.125\n",
            "Epoch 413, Loss: 1144773.25\n",
            "Epoch 414, Loss: 1059648.875\n",
            "Epoch 415, Loss: 983412.375\n",
            "Epoch 416, Loss: 938681.75\n",
            "Epoch 417, Loss: 922311.4375\n",
            "Epoch 418, Loss: 873086.4375\n",
            "Epoch 419, Loss: 842101.9375\n",
            "Epoch 420, Loss: 834586.5\n",
            "Epoch 421, Loss: 802702.875\n",
            "Epoch 422, Loss: 800983.25\n",
            "Epoch 423, Loss: 775656.625\n",
            "Epoch 424, Loss: 724030.8125\n",
            "Epoch 425, Loss: 688483.5625\n",
            "Epoch 426, Loss: 679720.625\n",
            "Epoch 427, Loss: 648362.9375\n",
            "Epoch 428, Loss: 624938.1875\n",
            "Epoch 429, Loss: 623713.0625\n",
            "Epoch 430, Loss: 592948.6875\n",
            "Epoch 431, Loss: 561913.1875\n",
            "Epoch 432, Loss: 561849.75\n",
            "Epoch 433, Loss: 546425.0625\n",
            "Epoch 434, Loss: 537111.375\n",
            "Epoch 435, Loss: 527724.9375\n",
            "Epoch 436, Loss: 527601.25\n",
            "Epoch 437, Loss: 512664.6875\n",
            "Epoch 438, Loss: 492457.125\n",
            "Epoch 439, Loss: 492428.28125\n",
            "Epoch 440, Loss: 483196.875\n",
            "Epoch 441, Loss: 473219.59375\n",
            "Epoch 442, Loss: 466372.34375\n",
            "Epoch 443, Loss: 463875.71875\n",
            "Epoch 444, Loss: 455744.40625\n",
            "Epoch 445, Loss: 445885.90625\n",
            "Epoch 446, Loss: 445672.59375\n",
            "Epoch 447, Loss: 445518.3125\n",
            "Epoch 448, Loss: 444548.5625\n",
            "Epoch 449, Loss: 444402.09375\n",
            "Epoch 450, Loss: 434474.15625\n",
            "Epoch 451, Loss: 434437.4375\n",
            "Epoch 452, Loss: 434432.625\n",
            "Epoch 453, Loss: 434431.0\n",
            "Epoch 454, Loss: 434422.84375\n",
            "Epoch 455, Loss: 434422.28125\n",
            "Epoch 456, Loss: 432840.4375\n",
            "Epoch 457, Loss: 347030.6875\n",
            "Epoch 458, Loss: 292005.78125\n",
            "Epoch 459, Loss: 246092.375\n",
            "Epoch 460, Loss: 217342.5\n",
            "Epoch 461, Loss: 208015.8125\n",
            "Epoch 462, Loss: 179721.390625\n",
            "Epoch 463, Loss: 172641.859375\n",
            "Epoch 464, Loss: 171073.53125\n",
            "Epoch 465, Loss: 160931.671875\n",
            "Epoch 466, Loss: 151819.703125\n",
            "Epoch 467, Loss: 151414.125\n",
            "Epoch 468, Loss: 151412.75\n",
            "Epoch 469, Loss: 142718.375\n",
            "Epoch 470, Loss: 128852.21875\n",
            "Epoch 471, Loss: 119463.375\n",
            "Epoch 472, Loss: 114644.328125\n",
            "Epoch 473, Loss: 114600.7265625\n",
            "Epoch 474, Loss: 114600.3046875\n",
            "Epoch 475, Loss: 114600.171875\n",
            "Epoch 476, Loss: 114600.125\n",
            "Epoch 477, Loss: 98119.1953125\n",
            "Epoch 478, Loss: 97657.046875\n",
            "Epoch 479, Loss: 97654.8984375\n",
            "Epoch 480, Loss: 97654.3671875\n",
            "Epoch 481, Loss: 90199.2734375\n",
            "Epoch 482, Loss: 90190.9296875\n",
            "Epoch 483, Loss: 90189.890625\n",
            "Epoch 484, Loss: 90189.53125\n",
            "Epoch 485, Loss: 90189.34375\n",
            "Epoch 486, Loss: 89955.4375\n",
            "Epoch 487, Loss: 89953.4296875\n",
            "Epoch 488, Loss: 89913.3046875\n",
            "Epoch 489, Loss: 89912.90625\n",
            "Epoch 490, Loss: 88671.5390625\n",
            "Epoch 491, Loss: 82946.484375\n",
            "Epoch 492, Loss: 82655.9375\n",
            "Epoch 493, Loss: 82650.21875\n",
            "Epoch 494, Loss: 82646.359375\n",
            "Epoch 495, Loss: 82645.75\n",
            "Epoch 496, Loss: 75315.859375\n",
            "Epoch 497, Loss: 74497.515625\n",
            "Epoch 498, Loss: 74382.84375\n",
            "Epoch 499, Loss: 74377.0390625\n",
            "Epoch 500, Loss: 74375.2734375\n",
            "Epoch 501, Loss: 74374.390625\n",
            "Epoch 502, Loss: 74373.8671875\n",
            "Epoch 503, Loss: 74373.515625\n",
            "Epoch 504, Loss: 74373.2734375\n",
            "Epoch 505, Loss: 74373.09375\n",
            "Epoch 506, Loss: 74372.953125\n",
            "Epoch 507, Loss: 74372.859375\n",
            "Epoch 508, Loss: 74372.78125\n",
            "Epoch 509, Loss: 74372.7109375\n",
            "Epoch 510, Loss: 74372.65625\n",
            "Epoch 511, Loss: 74372.6015625\n",
            "Epoch 512, Loss: 74372.5546875\n",
            "Epoch 513, Loss: 74372.53125\n",
            "Epoch 514, Loss: 74372.5\n",
            "Epoch 515, Loss: 74372.4765625\n",
            "Epoch 516, Loss: 74372.453125\n",
            "Epoch 517, Loss: 74372.4296875\n",
            "Epoch 518, Loss: 74372.4140625\n",
            "Epoch 519, Loss: 74372.40625\n",
            "Epoch 520, Loss: 73798.2421875\n",
            "Epoch 521, Loss: 73328.328125\n",
            "Epoch 522, Loss: 73310.4140625\n",
            "Epoch 523, Loss: 73308.6953125\n",
            "Epoch 524, Loss: 73308.1171875\n",
            "Epoch 525, Loss: 73307.828125\n",
            "Epoch 526, Loss: 58777.48828125\n",
            "Epoch 527, Loss: 58305.8984375\n",
            "Epoch 528, Loss: 58281.45703125\n",
            "Epoch 529, Loss: 58275.046875\n",
            "Epoch 530, Loss: 58272.390625\n",
            "Epoch 531, Loss: 58271.01953125\n",
            "Epoch 532, Loss: 58270.20703125\n",
            "Epoch 533, Loss: 58269.6875\n",
            "Epoch 534, Loss: 58269.31640625\n",
            "Epoch 535, Loss: 58269.05078125\n",
            "Epoch 536, Loss: 58268.85546875\n",
            "Epoch 537, Loss: 58268.70703125\n",
            "Epoch 538, Loss: 58268.58203125\n",
            "Epoch 539, Loss: 58268.48828125\n",
            "Epoch 540, Loss: 58268.40625\n",
            "Epoch 541, Loss: 58268.3359375\n",
            "Epoch 542, Loss: 58268.2890625\n",
            "Epoch 543, Loss: 58268.234375\n",
            "Epoch 544, Loss: 58268.19921875\n",
            "Epoch 545, Loss: 58268.16796875\n",
            "Epoch 546, Loss: 58268.12890625\n",
            "Epoch 547, Loss: 58268.1015625\n",
            "Epoch 548, Loss: 58268.0859375\n",
            "Epoch 549, Loss: 58268.06640625\n",
            "Epoch 550, Loss: 58268.04296875\n",
            "Epoch 551, Loss: 58268.02734375\n",
            "Epoch 552, Loss: 58268.01171875\n",
            "Epoch 553, Loss: 58267.9921875\n",
            "Epoch 554, Loss: 58267.98828125\n",
            "Epoch 555, Loss: 58267.9765625\n",
            "Epoch 556, Loss: 58267.9609375\n",
            "Epoch 557, Loss: 58267.94921875\n",
            "Epoch 558, Loss: 58267.9453125\n",
            "Epoch 559, Loss: 58267.9375\n",
            "Epoch 560, Loss: 58267.9296875\n",
            "Epoch 561, Loss: 58267.91796875\n",
            "Epoch 562, Loss: 58267.91796875\n",
            "Epoch 563, Loss: 58267.91796875\n",
            "Epoch 564, Loss: 58267.90625\n",
            "Epoch 565, Loss: 58267.90625\n",
            "Epoch 566, Loss: 58267.89453125\n",
            "Epoch 567, Loss: 58267.89453125\n",
            "Epoch 568, Loss: 58267.890625\n",
            "Epoch 569, Loss: 58267.88671875\n",
            "Epoch 570, Loss: 58267.87890625\n",
            "Epoch 571, Loss: 58267.875\n",
            "Epoch 572, Loss: 58267.875\n",
            "Epoch 573, Loss: 58267.8671875\n",
            "Epoch 574, Loss: 58267.8671875\n",
            "Epoch 575, Loss: 58267.86328125\n",
            "Epoch 576, Loss: 58267.86328125\n",
            "Epoch 577, Loss: 58267.859375\n",
            "Epoch 578, Loss: 58267.8515625\n",
            "Epoch 579, Loss: 58267.8515625\n",
            "Epoch 580, Loss: 58267.8515625\n",
            "Epoch 581, Loss: 58267.8515625\n",
            "Epoch 582, Loss: 58267.84765625\n",
            "Epoch 583, Loss: 58267.84765625\n",
            "Epoch 584, Loss: 58267.83984375\n",
            "Epoch 585, Loss: 58267.83984375\n",
            "Epoch 586, Loss: 58267.8359375\n",
            "Epoch 587, Loss: 58267.83984375\n",
            "Epoch 588, Loss: 58267.83203125\n",
            "Epoch 589, Loss: 58267.83203125\n",
            "Epoch 590, Loss: 58267.8203125\n",
            "Epoch 591, Loss: 58267.82421875\n",
            "Epoch 592, Loss: 58267.8203125\n",
            "Epoch 593, Loss: 58267.82421875\n",
            "Epoch 594, Loss: 58267.80859375\n",
            "Epoch 595, Loss: 58267.80859375\n",
            "Epoch 596, Loss: 58267.80859375\n",
            "Epoch 597, Loss: 58267.80859375\n",
            "Epoch 598, Loss: 58267.8046875\n",
            "Epoch 599, Loss: 58267.8046875\n",
            "Epoch 600, Loss: 58267.796875\n",
            "Epoch 601, Loss: 58267.796875\n",
            "Epoch 602, Loss: 58267.796875\n",
            "Epoch 603, Loss: 58267.796875\n",
            "Epoch 604, Loss: 58267.79296875\n",
            "Epoch 605, Loss: 58267.78515625\n",
            "Epoch 606, Loss: 58267.78515625\n",
            "Epoch 607, Loss: 58267.78125\n",
            "Epoch 608, Loss: 58267.77734375\n",
            "Epoch 609, Loss: 58267.77734375\n",
            "Epoch 610, Loss: 58267.78125\n",
            "Epoch 611, Loss: 58267.77734375\n",
            "Epoch 612, Loss: 58267.77734375\n",
            "Epoch 613, Loss: 58267.765625\n",
            "Epoch 614, Loss: 58267.77734375\n",
            "Epoch 615, Loss: 58267.76953125\n",
            "Epoch 616, Loss: 58267.765625\n",
            "Epoch 617, Loss: 58267.765625\n",
            "Epoch 618, Loss: 58267.7578125\n",
            "Epoch 619, Loss: 58267.7578125\n",
            "Epoch 620, Loss: 58267.7578125\n",
            "Epoch 621, Loss: 58267.75390625\n",
            "Epoch 622, Loss: 58267.75390625\n",
            "Epoch 623, Loss: 58267.75\n",
            "Epoch 624, Loss: 58267.75390625\n",
            "Epoch 625, Loss: 58267.75\n",
            "Epoch 626, Loss: 58267.75390625\n",
            "Epoch 627, Loss: 58267.7421875\n",
            "Epoch 628, Loss: 58267.7421875\n",
            "Epoch 629, Loss: 58267.7421875\n",
            "Epoch 630, Loss: 58267.73828125\n",
            "Epoch 631, Loss: 58267.73828125\n",
            "Epoch 632, Loss: 58267.73046875\n",
            "Epoch 633, Loss: 58267.73828125\n",
            "Epoch 634, Loss: 58267.73046875\n",
            "Epoch 635, Loss: 58267.73046875\n",
            "Epoch 636, Loss: 58267.7265625\n",
            "Epoch 637, Loss: 58267.73046875\n",
            "Epoch 638, Loss: 58267.73046875\n",
            "Epoch 639, Loss: 58267.72265625\n",
            "Epoch 640, Loss: 58267.72265625\n",
            "Epoch 641, Loss: 58267.72265625\n",
            "Epoch 642, Loss: 58267.72265625\n",
            "Epoch 643, Loss: 58267.72265625\n",
            "Epoch 644, Loss: 58267.71484375\n",
            "Epoch 645, Loss: 58267.7109375\n",
            "Epoch 646, Loss: 58267.7109375\n",
            "Epoch 647, Loss: 58267.7109375\n",
            "Epoch 648, Loss: 58267.7109375\n",
            "Epoch 649, Loss: 58267.7109375\n",
            "Epoch 650, Loss: 58267.703125\n",
            "Epoch 651, Loss: 58267.69921875\n",
            "Epoch 652, Loss: 58267.69921875\n",
            "Epoch 653, Loss: 58267.69921875\n",
            "Epoch 654, Loss: 58267.703125\n",
            "Epoch 655, Loss: 58267.69921875\n",
            "Epoch 656, Loss: 58267.69921875\n",
            "Epoch 657, Loss: 58267.69921875\n",
            "Epoch 658, Loss: 58267.6953125\n",
            "Epoch 659, Loss: 58267.6953125\n",
            "Epoch 660, Loss: 58267.6875\n",
            "Epoch 661, Loss: 58267.6875\n",
            "Epoch 662, Loss: 58267.6875\n",
            "Epoch 663, Loss: 58267.68359375\n",
            "Epoch 664, Loss: 58267.68359375\n",
            "Epoch 665, Loss: 58267.6796875\n",
            "Epoch 666, Loss: 58267.6796875\n",
            "Epoch 667, Loss: 58267.6796875\n",
            "Epoch 668, Loss: 58267.6796875\n",
            "Epoch 669, Loss: 58267.671875\n",
            "Epoch 670, Loss: 58267.671875\n",
            "Epoch 671, Loss: 58267.66796875\n",
            "Epoch 672, Loss: 58267.671875\n",
            "Epoch 673, Loss: 58267.66796875\n",
            "Epoch 674, Loss: 58267.66796875\n",
            "Epoch 675, Loss: 58267.66015625\n",
            "Epoch 676, Loss: 58267.66796875\n",
            "Epoch 677, Loss: 58267.66796875\n",
            "Epoch 678, Loss: 58267.66015625\n",
            "Epoch 679, Loss: 58267.66015625\n",
            "Epoch 680, Loss: 58267.65625\n",
            "Epoch 681, Loss: 58267.65234375\n",
            "Epoch 682, Loss: 58267.65625\n",
            "Epoch 683, Loss: 58267.65234375\n",
            "Epoch 684, Loss: 58267.65234375\n",
            "Epoch 685, Loss: 58267.64453125\n",
            "Epoch 686, Loss: 58267.65234375\n",
            "Epoch 687, Loss: 58267.64453125\n",
            "Epoch 688, Loss: 58267.64453125\n",
            "Epoch 689, Loss: 58267.64453125\n",
            "Epoch 690, Loss: 58267.64453125\n",
            "Epoch 691, Loss: 58267.640625\n",
            "Epoch 692, Loss: 58267.6328125\n",
            "Epoch 693, Loss: 58267.6328125\n",
            "Epoch 694, Loss: 58267.6328125\n",
            "Epoch 695, Loss: 58267.6328125\n",
            "Epoch 696, Loss: 58267.6328125\n",
            "Epoch 697, Loss: 58267.6328125\n",
            "Epoch 698, Loss: 58267.62890625\n",
            "Epoch 699, Loss: 58267.62890625\n",
            "Epoch 700, Loss: 58267.625\n",
            "Epoch 701, Loss: 58267.625\n",
            "Epoch 702, Loss: 58267.62890625\n",
            "Epoch 703, Loss: 58267.625\n",
            "Epoch 704, Loss: 58267.6171875\n",
            "Epoch 705, Loss: 58267.625\n",
            "Epoch 706, Loss: 58267.6171875\n",
            "Epoch 707, Loss: 58267.6171875\n",
            "Epoch 708, Loss: 58267.6171875\n",
            "Epoch 709, Loss: 58267.61328125\n",
            "Epoch 710, Loss: 58267.61328125\n",
            "Epoch 711, Loss: 58267.61328125\n",
            "Epoch 712, Loss: 58267.60546875\n",
            "Epoch 713, Loss: 58267.60546875\n",
            "Epoch 714, Loss: 58267.61328125\n",
            "Epoch 715, Loss: 58267.60546875\n",
            "Epoch 716, Loss: 58267.60546875\n",
            "Epoch 717, Loss: 58267.6015625\n",
            "Epoch 718, Loss: 58267.6015625\n",
            "Epoch 719, Loss: 58267.6015625\n",
            "Epoch 720, Loss: 58267.58984375\n",
            "Epoch 721, Loss: 58267.58984375\n",
            "Epoch 722, Loss: 58267.59765625\n",
            "Epoch 723, Loss: 58267.58984375\n",
            "Epoch 724, Loss: 58267.59765625\n",
            "Epoch 725, Loss: 58267.58984375\n",
            "Epoch 726, Loss: 58267.5859375\n",
            "Epoch 727, Loss: 58267.5859375\n",
            "Epoch 728, Loss: 58267.578125\n",
            "Epoch 729, Loss: 58267.5859375\n",
            "Epoch 730, Loss: 58267.5859375\n",
            "Epoch 731, Loss: 58267.578125\n",
            "Epoch 732, Loss: 58267.578125\n",
            "Epoch 733, Loss: 58267.578125\n",
            "Epoch 734, Loss: 58267.57421875\n",
            "Epoch 735, Loss: 58267.5859375\n",
            "Epoch 736, Loss: 58267.5859375\n",
            "Epoch 737, Loss: 58267.58984375\n",
            "Epoch 738, Loss: 58267.5859375\n",
            "Epoch 739, Loss: 58267.578125\n",
            "Epoch 740, Loss: 58267.578125\n",
            "Epoch 741, Loss: 58267.5703125\n",
            "Epoch 742, Loss: 58267.5703125\n",
            "Epoch 743, Loss: 58267.58984375\n",
            "Epoch 744, Loss: 58267.5625\n",
            "Epoch 745, Loss: 58267.5703125\n",
            "Epoch 746, Loss: 58267.5703125\n",
            "Epoch 747, Loss: 58267.57421875\n",
            "Epoch 748, Loss: 58267.5703125\n",
            "Epoch 749, Loss: 58267.57421875\n",
            "Epoch 750, Loss: 58267.5703125\n",
            "Epoch 751, Loss: 58267.55859375\n",
            "Epoch 752, Loss: 58267.55859375\n",
            "Epoch 753, Loss: 58267.55859375\n",
            "Epoch 754, Loss: 58267.55078125\n",
            "Epoch 755, Loss: 58267.55859375\n",
            "Epoch 756, Loss: 58267.55859375\n",
            "Epoch 757, Loss: 58267.55078125\n",
            "Epoch 758, Loss: 58267.55078125\n",
            "Epoch 759, Loss: 58267.546875\n",
            "Epoch 760, Loss: 58267.54296875\n",
            "Epoch 761, Loss: 58267.546875\n",
            "Epoch 762, Loss: 58267.546875\n",
            "Epoch 763, Loss: 58267.546875\n",
            "Epoch 764, Loss: 58267.55078125\n",
            "Epoch 765, Loss: 58267.55078125\n",
            "Epoch 766, Loss: 58267.55859375\n",
            "Epoch 767, Loss: 58267.55078125\n",
            "Epoch 768, Loss: 58267.546875\n",
            "Epoch 769, Loss: 58267.54296875\n",
            "Epoch 770, Loss: 58267.53515625\n",
            "Epoch 771, Loss: 58267.53515625\n",
            "Epoch 772, Loss: 58267.53515625\n",
            "Epoch 773, Loss: 58267.5234375\n",
            "Epoch 774, Loss: 58267.53515625\n",
            "Epoch 775, Loss: 58267.53125\n",
            "Epoch 776, Loss: 58267.5234375\n",
            "Epoch 777, Loss: 58267.5234375\n",
            "Epoch 778, Loss: 58267.5234375\n",
            "Epoch 779, Loss: 58267.5234375\n",
            "Epoch 780, Loss: 58267.51953125\n",
            "Epoch 781, Loss: 58267.5234375\n",
            "Epoch 782, Loss: 58267.5234375\n",
            "Epoch 783, Loss: 58267.51953125\n",
            "Epoch 784, Loss: 58267.515625\n",
            "Epoch 785, Loss: 58267.51953125\n",
            "Epoch 786, Loss: 58267.51953125\n",
            "Epoch 787, Loss: 58267.515625\n",
            "Epoch 788, Loss: 58267.515625\n",
            "Epoch 789, Loss: 58267.515625\n",
            "Epoch 790, Loss: 58267.515625\n",
            "Epoch 791, Loss: 58267.515625\n",
            "Epoch 792, Loss: 58267.515625\n",
            "Epoch 793, Loss: 58267.5078125\n",
            "Epoch 794, Loss: 58267.50390625\n",
            "Epoch 795, Loss: 58267.50390625\n",
            "Epoch 796, Loss: 58267.50390625\n",
            "Epoch 797, Loss: 58267.50390625\n",
            "Epoch 798, Loss: 58267.49609375\n",
            "Epoch 799, Loss: 58267.49609375\n",
            "Epoch 800, Loss: 58267.49609375\n",
            "Epoch 801, Loss: 58267.4921875\n",
            "Epoch 802, Loss: 58267.4921875\n",
            "Epoch 803, Loss: 58267.49609375\n",
            "Epoch 804, Loss: 58267.49609375\n",
            "Epoch 805, Loss: 58267.49609375\n",
            "Epoch 806, Loss: 58267.4921875\n",
            "Epoch 807, Loss: 58267.4921875\n",
            "Epoch 808, Loss: 58267.48828125\n",
            "Epoch 809, Loss: 58267.48046875\n",
            "Epoch 810, Loss: 58267.4921875\n",
            "Epoch 811, Loss: 58267.48828125\n",
            "Epoch 812, Loss: 58267.48828125\n",
            "Epoch 813, Loss: 58267.48046875\n",
            "Epoch 814, Loss: 58267.48828125\n",
            "Epoch 815, Loss: 58267.48828125\n",
            "Epoch 816, Loss: 58267.48046875\n",
            "Epoch 817, Loss: 58267.48046875\n",
            "Epoch 818, Loss: 58267.4765625\n",
            "Epoch 819, Loss: 58267.48046875\n",
            "Epoch 820, Loss: 58267.48046875\n",
            "Epoch 821, Loss: 58267.48046875\n",
            "Epoch 822, Loss: 58267.48046875\n",
            "Epoch 823, Loss: 58267.4765625\n",
            "Epoch 824, Loss: 58267.46875\n",
            "Epoch 825, Loss: 58267.4765625\n",
            "Epoch 826, Loss: 58267.46875\n",
            "Epoch 827, Loss: 58267.46875\n",
            "Epoch 828, Loss: 58267.46875\n",
            "Epoch 829, Loss: 58267.46875\n",
            "Epoch 830, Loss: 58267.46875\n",
            "Epoch 831, Loss: 58267.46875\n",
            "Epoch 832, Loss: 58267.46484375\n",
            "Epoch 833, Loss: 58267.46875\n",
            "Epoch 834, Loss: 58267.4609375\n",
            "Epoch 835, Loss: 58267.46484375\n",
            "Epoch 836, Loss: 58267.46484375\n",
            "Epoch 837, Loss: 58267.46484375\n",
            "Epoch 838, Loss: 58267.46484375\n",
            "Epoch 839, Loss: 58267.4609375\n",
            "Epoch 840, Loss: 58267.4609375\n",
            "Epoch 841, Loss: 58267.4609375\n",
            "Epoch 842, Loss: 58267.4609375\n",
            "Epoch 843, Loss: 58267.4609375\n",
            "Epoch 844, Loss: 58267.4609375\n",
            "Epoch 845, Loss: 58267.44921875\n",
            "Epoch 846, Loss: 58267.453125\n",
            "Epoch 847, Loss: 58267.4609375\n",
            "Epoch 848, Loss: 58267.44921875\n",
            "Epoch 849, Loss: 58267.44921875\n",
            "Epoch 850, Loss: 58267.44921875\n",
            "Epoch 851, Loss: 58267.44921875\n",
            "Epoch 852, Loss: 58267.44921875\n",
            "Epoch 853, Loss: 58267.44921875\n",
            "Epoch 854, Loss: 58267.4453125\n",
            "Epoch 855, Loss: 58267.4375\n",
            "Epoch 856, Loss: 58267.4453125\n",
            "Epoch 857, Loss: 58267.4453125\n",
            "Epoch 858, Loss: 58267.4453125\n",
            "Epoch 859, Loss: 58267.4375\n",
            "Epoch 860, Loss: 58267.4375\n",
            "Epoch 861, Loss: 58267.4375\n",
            "Epoch 862, Loss: 58267.4375\n",
            "Epoch 863, Loss: 58267.4375\n",
            "Epoch 864, Loss: 58267.4375\n",
            "Epoch 865, Loss: 58267.43359375\n",
            "Epoch 866, Loss: 58267.4375\n",
            "Epoch 867, Loss: 58267.43359375\n",
            "Epoch 868, Loss: 58267.4375\n",
            "Epoch 869, Loss: 58267.43359375\n",
            "Epoch 870, Loss: 58267.43359375\n",
            "Epoch 871, Loss: 58267.42578125\n",
            "Epoch 872, Loss: 58267.42578125\n",
            "Epoch 873, Loss: 58267.43359375\n",
            "Epoch 874, Loss: 58267.42578125\n",
            "Epoch 875, Loss: 58267.43359375\n",
            "Epoch 876, Loss: 58267.42578125\n",
            "Epoch 877, Loss: 58267.42578125\n",
            "Epoch 878, Loss: 58267.421875\n",
            "Epoch 879, Loss: 58267.42578125\n",
            "Epoch 880, Loss: 58267.421875\n",
            "Epoch 881, Loss: 58267.421875\n",
            "Epoch 882, Loss: 58267.421875\n",
            "Epoch 883, Loss: 58267.41796875\n",
            "Epoch 884, Loss: 58267.41796875\n",
            "Epoch 885, Loss: 58267.41796875\n",
            "Epoch 886, Loss: 58267.41015625\n",
            "Epoch 887, Loss: 58267.41796875\n",
            "Epoch 888, Loss: 58267.41796875\n",
            "Epoch 889, Loss: 58267.41796875\n",
            "Epoch 890, Loss: 58267.41015625\n",
            "Epoch 891, Loss: 58267.41015625\n",
            "Epoch 892, Loss: 58267.41015625\n",
            "Epoch 893, Loss: 58267.41015625\n",
            "Epoch 894, Loss: 58267.40625\n",
            "Epoch 895, Loss: 58267.40625\n",
            "Epoch 896, Loss: 58267.40625\n",
            "Epoch 897, Loss: 58267.40625\n",
            "Epoch 898, Loss: 58267.41015625\n",
            "Epoch 899, Loss: 58267.41796875\n",
            "Epoch 900, Loss: 58267.41796875\n",
            "Epoch 901, Loss: 58267.41015625\n",
            "Epoch 902, Loss: 58267.41015625\n",
            "Epoch 903, Loss: 58267.41015625\n",
            "Epoch 904, Loss: 58267.3984375\n",
            "Epoch 905, Loss: 58267.3984375\n",
            "Epoch 906, Loss: 58267.40625\n",
            "Epoch 907, Loss: 58267.39453125\n",
            "Epoch 908, Loss: 58267.40625\n",
            "Epoch 909, Loss: 58267.40625\n",
            "Epoch 910, Loss: 58267.39453125\n",
            "Epoch 911, Loss: 58267.39453125\n",
            "Epoch 912, Loss: 58267.39453125\n",
            "Epoch 913, Loss: 58267.39453125\n",
            "Epoch 914, Loss: 58267.39453125\n",
            "Epoch 915, Loss: 58267.390625\n",
            "Epoch 916, Loss: 58267.39453125\n",
            "Epoch 917, Loss: 58267.39453125\n",
            "Epoch 918, Loss: 58267.390625\n",
            "Epoch 919, Loss: 58267.3828125\n",
            "Epoch 920, Loss: 58267.3828125\n",
            "Epoch 921, Loss: 58267.3828125\n",
            "Epoch 922, Loss: 58267.3828125\n",
            "Epoch 923, Loss: 58267.3828125\n",
            "Epoch 924, Loss: 58267.3828125\n",
            "Epoch 925, Loss: 58267.3828125\n",
            "Epoch 926, Loss: 58267.3828125\n",
            "Epoch 927, Loss: 58267.3828125\n",
            "Epoch 928, Loss: 58267.390625\n",
            "Epoch 929, Loss: 58267.390625\n",
            "Epoch 930, Loss: 58267.3828125\n",
            "Epoch 931, Loss: 58267.390625\n",
            "Epoch 932, Loss: 58267.3828125\n",
            "Epoch 933, Loss: 58267.37109375\n",
            "Epoch 934, Loss: 58267.37109375\n",
            "Epoch 935, Loss: 58267.37109375\n",
            "Epoch 936, Loss: 58267.37109375\n",
            "Epoch 937, Loss: 58267.37109375\n",
            "Epoch 938, Loss: 58267.37109375\n",
            "Epoch 939, Loss: 58267.37109375\n",
            "Epoch 940, Loss: 58267.37109375\n",
            "Epoch 941, Loss: 58267.3671875\n",
            "Epoch 942, Loss: 58267.37109375\n",
            "Epoch 943, Loss: 58267.37109375\n",
            "Epoch 944, Loss: 58267.3671875\n",
            "Epoch 945, Loss: 58267.37109375\n",
            "Epoch 946, Loss: 58267.3671875\n",
            "Epoch 947, Loss: 58267.3671875\n",
            "Epoch 948, Loss: 58267.36328125\n",
            "Epoch 949, Loss: 58267.3671875\n",
            "Epoch 950, Loss: 58267.36328125\n",
            "Epoch 951, Loss: 58267.3671875\n",
            "Epoch 952, Loss: 58267.36328125\n",
            "Epoch 953, Loss: 58267.36328125\n",
            "Epoch 954, Loss: 58267.36328125\n",
            "Epoch 955, Loss: 58267.36328125\n",
            "Epoch 956, Loss: 58267.36328125\n",
            "Epoch 957, Loss: 58267.36328125\n",
            "Epoch 958, Loss: 58267.35546875\n",
            "Epoch 959, Loss: 58267.36328125\n",
            "Epoch 960, Loss: 58267.35546875\n",
            "Epoch 961, Loss: 58267.35546875\n",
            "Epoch 962, Loss: 58267.35546875\n",
            "Epoch 963, Loss: 58267.35546875\n",
            "Epoch 964, Loss: 58267.36328125\n",
            "Epoch 965, Loss: 58267.36328125\n",
            "Epoch 966, Loss: 58267.3515625\n",
            "Epoch 967, Loss: 58267.3515625\n",
            "Epoch 968, Loss: 58267.3515625\n",
            "Epoch 969, Loss: 58267.35546875\n",
            "Epoch 970, Loss: 58267.34375\n",
            "Epoch 971, Loss: 58267.3515625\n",
            "Epoch 972, Loss: 58267.3515625\n",
            "Epoch 973, Loss: 58267.3515625\n",
            "Epoch 974, Loss: 58267.34375\n",
            "Epoch 975, Loss: 58267.33984375\n",
            "Epoch 976, Loss: 58267.33984375\n",
            "Epoch 977, Loss: 58267.34375\n",
            "Epoch 978, Loss: 58267.33984375\n",
            "Epoch 979, Loss: 58267.33984375\n",
            "Epoch 980, Loss: 58267.33984375\n",
            "Epoch 981, Loss: 58267.34375\n",
            "Epoch 982, Loss: 58267.33984375\n",
            "Epoch 983, Loss: 58267.3359375\n",
            "Epoch 984, Loss: 58267.33984375\n",
            "Epoch 985, Loss: 58267.3359375\n",
            "Epoch 986, Loss: 58267.3359375\n",
            "Epoch 987, Loss: 58267.328125\n",
            "Epoch 988, Loss: 58267.328125\n",
            "Epoch 989, Loss: 58267.328125\n",
            "Epoch 990, Loss: 58267.328125\n",
            "Epoch 991, Loss: 58267.328125\n",
            "Epoch 992, Loss: 58267.32421875\n",
            "Epoch 993, Loss: 58267.328125\n",
            "Epoch 994, Loss: 58267.32421875\n",
            "Epoch 995, Loss: 58267.328125\n",
            "Epoch 996, Loss: 58267.32421875\n",
            "Epoch 997, Loss: 58267.328125\n",
            "Epoch 998, Loss: 58267.32421875\n",
            "Epoch 999, Loss: 58267.32421875\n",
            "Epoch 1000, Loss: 58267.328125\n"
          ]
        }
      ],
      "source": [
        "args = argparse.ArgumentParser(description='Process some integers.')\n",
        "args.scenario = \"intersection\"\n",
        "args.restore = False\n",
        "args.goal = \"left\"\n",
        "args.epochs = 1000\n",
        "args.lr = 0.0002\n",
        "data = load_data(args)\n",
        "nn(data, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1065c665",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1065c665",
        "outputId": "aacfd942-06ed-4ab6-c420-af17405558c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 12 05:48:47 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    27W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mt8pRy96Wvya"
      },
      "id": "Mt8pRy96Wvya",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}