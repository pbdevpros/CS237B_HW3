{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd3bb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd\n",
    "import argparse\n",
    "from utils import *\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07555ce",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c52127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn(data, args):\n",
    "    \"\"\"\n",
    "    Trains a feedforward NN. \n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'train_batch_size': 4096*32,\n",
    "    }\n",
    "    in_size = data['x_train'].shape[-1]\n",
    "    out_size = data['y_train'].shape[-1]\n",
    "    \n",
    "    nn_model = NN(in_size, out_size)\n",
    "    if args.restore:\n",
    "        nn_model.load_weights('./policies/' + args.scenario.lower() + '_' + args.goal.lower() + '_ILDIST')\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=args.lr)\n",
    "\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(x, y):\n",
    "        ######### Your code starts here #########\n",
    "        # We want to perform a single training step (for one batch):\n",
    "        # 1. Make a forward pass through the model\n",
    "        # 2. Calculate the loss for the output of the forward pass\n",
    "        # 3. Based on the loss calculate the gradient for all weights\n",
    "        # 4. Run an optimization step on the weights.\n",
    "        # Helpful Functions: tf.GradientTape(), tf.GradientTape.gradient(), tf.keras.Optimizer.apply_gradients\n",
    "        with tf.GradientTape() as tape:\n",
    "            # forward pass\n",
    "            y_est = nn_model(x, training=True) # use dropout\n",
    "            # compute the loss\n",
    "            current_loss = loss(y_est, y)\n",
    "        grads = tape.gradient(current_loss, nn_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, nn_model.trainable_variables))\n",
    "        ########## Your code ends here ##########\n",
    "\n",
    "        train_loss(current_loss)\n",
    "\n",
    "    @tf.function\n",
    "    def train(train_data):\n",
    "        for x, y in train_data:\n",
    "            train_step(x, y)\n",
    "\n",
    "\n",
    "    train_data = tf.data.Dataset.from_tensor_slices((data['x_train'], data['y_train'])).shuffle(100000).batch(params['train_batch_size'])\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "\n",
    "        train(train_data)\n",
    "\n",
    "        template = 'Epoch {}, Loss: {}'\n",
    "        print(template.format(epoch + 1, train_loss.result()))\n",
    "    nn_model.save_weights('./policies/' + args.scenario.lower() + '_' + args.goal.lower() + '_ILDIST')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e6db4a",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c1573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixtureDensityModelErrorFinal(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MixtureDensityModelErrorFinal, self).__init__()\n",
    "\n",
    "    def call(self, y_true, y_pred, sample_weight=None):\n",
    "        y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "        self.z_mu = y_pred[:, :2]\n",
    "        self.z_sigma = y_pred[:, 2:]\n",
    "        epsilon = 0.00001\n",
    "        # print(self.z_mu.shape)\n",
    "        # print(self.z_sigma.shape)\n",
    "        # B, N = self.z_sigma.shape\n",
    "        # self.z_sigma = tf.reshape(self.z_sigma, (B, int(N/2), int(N/2)))\n",
    "        # covariance = self.z_sigma @ tf.transpose(self.z_sigma, perm=[0, 2, 1])\n",
    "        scale_tril = tfp.math.fill_triangular(self.z_sigma) + epsilon\n",
    "        # sigma = tf.matmul(scale_tril, tf.transpose(scale_tril, perm=[0, 2, 1]))\n",
    "        # print(covariance.shape)\n",
    "        mvn = tfd.MultivariateNormalTriL(loc=self.z_mu, scale_tril=scale_tril, allow_nan_stats=False)\n",
    "        # E = tf.reduce_mean(tf.math.log(mvn.prob(y_true)), 0)\n",
    "        E = tf.reduce_mean(mvn.log_prob(y_true), 0)\n",
    "        return -1 * E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edec096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_est, y):\n",
    "    y = tf.cast(y, dtype=tf.float32)\n",
    "    ######### Your code starts here #########\n",
    "    # We want to compute the negative log-likelihood loss between y_est and y where\n",
    "    # - y_est is the output of the network for a batch of observations,\n",
    "    # - y is the actions the expert took for the corresponding batch of observations\n",
    "    # At the end your code should return the scalar loss value.\n",
    "    # HINT: You may find the classes of tensorflow_probability.distributions (imported as tfd) useful.\n",
    "    #       In particular, you can use MultivariateNormalFullCovariance or MultivariateNormalTriL, but they are not the only way.\n",
    "    # loss_object = MixtureDensityModelError(num_means=2, num_kernels=3)\n",
    "    loss_object = MixtureDensityModelErrorFinal()\n",
    "    sample_weights = tf.constant(([0.8, 0.2]))\n",
    "    y = y * sample_weights\n",
    "    y_est = y_est * tf.constant(([0.8, 0.2, 0.1, 0.1, 0.1]))\n",
    "    return loss_object(y, y_est)\n",
    "    \n",
    "    ########## Your code ends here ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6b1d42",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7c8641",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(tf.keras.Model):\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(NN, self).__init__()\n",
    "        \n",
    "        ######### Your code starts here #########\n",
    "        # We want to define and initialize the weights & biases of the neural network.\n",
    "        # - in_size is dim(O)\n",
    "        # - out_size is dim(A) = 2\n",
    "        # IMPORTANT: out_size is still 2 in this case, because the action space is 2-dimensional. But your network will output some other size as it is outputing a distribution!\n",
    "        # HINT: You should use either of the following for weight initialization:\n",
    "        #         - tf.keras.initializers.GlorotUniform (this is what we tried)\n",
    "        #         - tf.keras.initializers.GlorotNormal\n",
    "        #         - tf.keras.initializers.he_uniform or tf.keras.initializers.he_normal\n",
    "        self.internal_layers = [\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(24, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu'),\n",
    "            # tf.keras.layers.Dropout(0.1),\n",
    "            tf.keras.layers.Dense(24, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu'),\n",
    "            # tf.keras.layers.Dropout(0.2),\n",
    "            # tf.keras.layers.Dense(12, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu'),\n",
    "        ]\n",
    "        # num_outputs = (out_size + 2) * 3 # Removed after using only 6 ouputs\n",
    "        num_outputs = out_size + 3\n",
    "        self.layer_output = tf.keras.layers.Dense(num_outputs, kernel_initializer=tf.keras.initializers.GlorotUniform(), activation='relu')\n",
    "        ########## Your code ends here ##########\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.cast(x, dtype=tf.float32)\n",
    "        ######### Your code starts here #########\n",
    "        # We want to perform a forward-pass of the network. Using the weights and biases, this function should give the network output for x where:\n",
    "        # x is a (?, |O|) tensor that keeps a batch of observations\n",
    "        # IMPORTANT: First two columns of the output tensor must correspond to the mean vector!\n",
    "        for i in range(len(self.internal_layers)):\n",
    "            layer = self.internal_layers[i]\n",
    "            x = layer(x)\n",
    "        return self.layer_output(x)\n",
    "        ########## Your code ends here ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9de95d",
   "metadata": {},
   "source": [
    "# Run Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceb1fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser\n",
    "args.scenario = \"intersection\"\n",
    "args.restore = False\n",
    "args.goal = \"left\"\n",
    "args.epochs = 3\n",
    "args.lr = 0.0002\n",
    "data = load_data(args)\n",
    "# nn(data, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1065c665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
